{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random as rand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    s = 1.0/(1.0 + np.exp(-x))\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "\n",
    "    e = np.exp(x)\n",
    "    s = np.divide(e, np.sum(e, axis=0))\n",
    "\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoding(y, m):\n",
    "\n",
    "    encoded_matrix = np.zeros((m, 10))\n",
    "\n",
    "    for i in range(m):\n",
    "        # print(y[0, i])\n",
    "        encoded_matrix[i, y[0, i]] = 1.0\n",
    "      \n",
    "    return encoded_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(w1, w2, b1, b2, a0):\n",
    "    z1 = np.dot(w1, a0) + b1\n",
    "    a1 = sigmoid(z1)\n",
    "\n",
    "    z2 = np.dot(w2, a1) + b2\n",
    "    a2 = sigmoid(z2)\n",
    "\n",
    "    return a1, a2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation(a1, a2, x, y, m, w2):\n",
    "\n",
    "    dz2 = a2 - y\n",
    "    dw2 = (1.0/m)*np.dot(dz2, a1.T)\n",
    "    db2 = (1.0/m) * np.sum(dz2, axis=1, keepdims=True)\n",
    "\n",
    "    dz1 = np.dot(w2.T, dz2)*(a1*(1-a1))\n",
    "    dw1 = (1.0/m) * np.dot(dz1, x.T)\n",
    "    db1 = (1.0/m) * np.sum(dz1, axis=1, keepdims=True)\n",
    "\n",
    "    return dw1, dw2, db1, db2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_accuracy(actual, test, w1, w2, b1, b2, m):\n",
    "\n",
    "    a1m, a2m = forward_propagation(w1, w2, b1, b2, test)\n",
    "    sm = softmax(a2m)\n",
    "    \n",
    "    testing = np.zeros(sm.shape).T\n",
    "\n",
    "    max_val_pos = np.argmax(sm, axis=0)\n",
    "    for i in range(m):\n",
    "        testing[i, max_val_pos[i]] = 1\n",
    "\n",
    "    num_correct = np.sum(np.all(testing == actual, axis=1))\n",
    "    return (num_correct/m)*100    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost for alpha 0.1 with epochs 10000: 0.30799505364495006\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for training data 96.28571428571429\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for test data 92.86666666666666\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost for alpha 0.075 with epochs 40000: 0.10525777108224572\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for training data 99.45714285714286\nAccuracy for test data 93.0\n\n"
     ]
    }
   ],
   "source": [
    "def gradient_descent(epochs, alpha):\n",
    "    data = pd.read_csv('ex3_train.csv')\n",
    "    x = data.drop('y', axis=1).T\n",
    "    y = data['y'].values.reshape(1, -1)\n",
    "    m = y.shape[1]\n",
    "\n",
    "    cost = []\n",
    "    \n",
    "    # init weights and bias\n",
    "    np.random.seed(1)\n",
    "    # w1 = np.random.rand(25, 400) * 0.01\n",
    "    w1 = np.random.uniform(-1, 1, (25, 400)) * 0.01 \n",
    "    b1 = np.zeros((25, 1))\n",
    "    # b1 = np.ones((25, 1))\n",
    "    # w2 = np.random.rand(10, 25) * 0.01\n",
    "    w2 = np.random.uniform(-1, 1, (10, 25)) * 0.01\n",
    "    # b2 = np.ones((10, 1))\n",
    "    b2 = np.zeros((10, 1))\n",
    "\n",
    "    y = one_hot_encoding(y, y.shape[1]).T\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        a1, a2 = forward_propagation(w1, w2, b1, b2, x)\n",
    "\n",
    "        loss = -(1.0/m) * np.sum(np.multiply(y, np.log(a2)) + np.multiply(1.0 - y, np.log(1 - a2))) \n",
    "\n",
    "        loss = np.squeeze(loss)\n",
    "        cost.append(loss)\n",
    "         \n",
    "        dw1, dw2, db1, db2 = backward_propagation(a1, a2, x, y, m, w2)\n",
    "\n",
    "        w1 = w1 - (alpha * dw1)\n",
    "        b1 = b1 - (alpha * db1)\n",
    "        w2 = w2 - (alpha * dw2)\n",
    "        b2 = b2 - (alpha * db2)\n",
    "\n",
    "    plt.ylabel('Cost')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.plot(np.squeeze(cost), label='alpha {}'.format(alpha))\n",
    "    \n",
    "    # print(cost[-1])\n",
    "    print('Cost for alpha {} with epochs {}: {}'.format(alpha, epochs, cost[-1]))\n",
    "    \n",
    "    # test training data\n",
    "    train_data = pd.read_csv('ex3_train.csv')\n",
    "    train = train_data.drop('y', axis=1).T\n",
    "    actual_vals = train_data['y'].values.reshape(1, -1)\n",
    "    actual_vals = one_hot_encoding(actual_vals, m)\n",
    "    acc = calc_accuracy(actual_vals, train, w1, w2, b1, b2, m)\n",
    "    print('Accuracy for training data {}'.format(acc))\n",
    "    \n",
    "    # test test data\n",
    "    test_data = pd.read_csv('ex3_test.csv')\n",
    "    test = test_data.drop('y', axis=1).T\n",
    "    actual_vals = test_data['y'].values.reshape(1, -1)\n",
    "    m = actual_vals.shape[1]\n",
    "    actual_vals = one_hot_encoding(actual_vals, m)\n",
    "    acc = calc_accuracy(actual_vals, test, w1, w2, b1, b2, m)\n",
    "    print('Accuracy for test data {}\\n'.format(acc))\n",
    "    return acc\n",
    "\n",
    "\n",
    "# gradient_descent(alpha=0.5, epochs=7500)\n",
    "acc1 = gradient_descent(alpha=0.1, epochs=10000)\n",
    "acc2 = gradient_descent(alpha=0.075, epochs=40000)\n",
    "acc3 = gradient_descent(alpha=0.03, epochs=50000) \n",
    "# gradient_descent(alpha=0.05, epochs=50000) 93.26\n",
    "# gradient_descent(alpha=0.0, epochs=15000)\n",
    "acc_list = {0.1: acc1, 0.075: acc2, 0.03: acc3}\n",
    "# \n",
    "plt.legend(['alpha 0.1', 'alpha 0.075', 'alpha 0.03'], loc='upper right')\n",
    "plt.show()\n",
    "# \n",
    "best = max(acc_list, key=acc_list.get)\n",
    "print('Best test accuracy {} with alpha {}'.format(acc_list.get(best), best))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
